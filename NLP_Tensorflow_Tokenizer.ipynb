{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Tensorflow_Tokenizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlqSCRhnLUeio8jHYg8El5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitrgupta1/Deep-Learning/blob/master/NLP_Tensorflow_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_CTMOx0gSwF",
        "colab_type": "text"
      },
      "source": [
        "**Natural Language Processing With Tensorflow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDwIzrwDgdCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il51hSOLgk9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = ['i love my india', 'I love USA', 'I, love both India and usa!' ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUHZRNTphb6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99cb73e1-fc32-4fcd-c068-fb99fbf3a582"
      },
      "source": [
        "#instantiating Tokenizer with 100 words\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "\n",
        "#fitting the tokenizer on the text data\n",
        "\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "# Generating word index over the data\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# printing out the word index to see the indexing of each of the words of the text processed by the Tokenizer\n",
        "# Note : Look closely and you would find that the word indexes doesn't have the punctuation\n",
        "# Also the tokenizer lowercased all the words before indexing on them. Hence 'I' and 'i' are indxed as  'i'\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'india': 3, 'usa': 4, 'my': 5, 'both': 6, 'and': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjU1KEgXhjh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e3b6251-c84c-4bb3-c691-6330f1e45c1f"
      },
      "source": [
        "# Generating sequences corrsponding to the text sentences in data\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "# printing out the sequences formed\n",
        "\n",
        "# Notice, that the words are now replacced with the corresponding indexes in the sequence\n",
        "print(sequences)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 5, 3], [1, 2, 4], [1, 2, 6, 3, 7, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHYKPDtbhzZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b79a8150-247e-48f2-dc8d-5e8050da15fb"
      },
      "source": [
        "# Let's check what would happen if we generate some sequences for out test data from the tokenizer learned on our data\n",
        "\n",
        "test_data = ['I really love India', 'Everyone love usa and india as well']\n",
        "\n",
        "# Note that the sentences have words which were not present earlier in our data\n",
        "\n",
        "test_sequence = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# printing the test sequence\n",
        "\n",
        "print(test_sequence)\n",
        "\n",
        "# notice the new sequence is missing  the indexes coreesponding to the new word which were not present in our original data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3], [2, 4, 7, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPd6B-zRh3nx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9324b34d-9652-45ea-a03b-7c6b56f5203b"
      },
      "source": [
        "# Using the 'out of vocalbulary' token for the missing words\n",
        "\n",
        "#instantiating Tokenizer with 100 words\n",
        "oov_tokenizer = Tokenizer(num_words=100, oov_token='oov')\n",
        "\n",
        "#fitting the tokenizer on the text data\n",
        "\n",
        "oov_tokenizer.fit_on_texts(data)\n",
        "\n",
        "# Generating word index over the data\n",
        "\n",
        "word_index = oov_tokenizer.word_index\n",
        "\n",
        "# printing out the word index to see the indexing of each of the words of the text processed by the Tokenizer\n",
        "# Note : Look closely and you would find that the word indexes doesn't have the punctuation\n",
        "# Also the tokenizer lowercased all the words before indexing on them. Hence 'I' and 'i' are indxed as  'i'\n",
        "print(word_index)\n",
        "\n",
        "oov_sequences = oov_tokenizer.texts_to_sequences(data)\n",
        "\n",
        "# printing out the sequences formed\n",
        "\n",
        "# Notice, that the words are now replacced with the corresponding indexes in the sequence\n",
        "print(oov_sequences)\n",
        "\n",
        "# Let's check what would happen if we generate some sequences for out test data from the tokenizer learned on our data\n",
        "\n",
        "test_data = ['I really love India', 'Everyone love usa and india as well']\n",
        "\n",
        "# Note that the sentences have words which were not present earlier in our data\n",
        "\n",
        "oov_test_sequence = oov_tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# printing the test sequence\n",
        "\n",
        "print(oov_test_sequence)\n",
        "\n",
        "# Notice the oov token assigned 1 as index\n",
        "# also now the missing words are referenced as oov index(i.e., 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'oov': 1, 'i': 2, 'love': 3, 'india': 4, 'usa': 5, 'my': 6, 'both': 7, 'and': 8}\n",
            "[[2, 3, 6, 4], [2, 3, 5], [2, 3, 7, 4, 8, 5]]\n",
            "[[2, 1, 3, 4], [1, 3, 5, 8, 4, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCdmYisEn6Rl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "0b84095f-7d30-4788-b456-dec2c3264d86"
      },
      "source": [
        "# adding pad to the sequences for making them of the same length for our network\n",
        "\n",
        "padded_sequence = pad_sequences(oov_sequences)\n",
        "\n",
        "print('padded_sequences = \\n',padded_sequence)\n",
        "\n",
        "# in case you want pad the sequences at the end the write it as below\n",
        "# if you want to control the size of the sequences you can use the maxlen parameter for it.\n",
        "# Note that the sequence is dropped from the  start\n",
        "# you can add the parameter \"trucating='post' for dropping from the back\"\n",
        "print('==============================')\n",
        "post_pad_sequences = pad_sequences(oov_sequences, padding='post', maxlen=5)\n",
        "print('post_pad_sequences = \\n',post_pad_sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "padded_sequences = \n",
            " [[0 0 2 3 6 4]\n",
            " [0 0 0 2 3 5]\n",
            " [2 3 7 4 8 5]]\n",
            "==============================\n",
            "post_pad_sequences = \n",
            " [[2 3 6 4 0]\n",
            " [2 3 5 0 0]\n",
            " [3 7 4 8 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxXcXRGrp-3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}